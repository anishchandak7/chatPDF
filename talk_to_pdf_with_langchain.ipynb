{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 6745k    0 16384    0     0   9879      0  0:11:39  0:00:01  0:11:38  9887\n",
      "  0 6745k    0 65536    0     0  25898      0  0:04:26  0:00:02  0:04:24 25913\n",
      "  2 6745k    2  160k    0     0  48100      0  0:02:23  0:00:03  0:02:20 48117\n",
      "  5 6745k    5  384k    0     0  90296      0  0:01:16  0:00:04  0:01:12 90332\n",
      "  9 6745k    9  656k    0     0   123k      0  0:00:54  0:00:05  0:00:49  130k\n",
      " 15 6745k   15 1040k    0     0   164k      0  0:00:40  0:00:06  0:00:34  220k\n",
      " 22 6745k   22 1536k    0     0   209k      0  0:00:32  0:00:07  0:00:25  305k\n",
      " 28 6745k   28 1920k    0     0   230k      0  0:00:29  0:00:08  0:00:21  357k\n",
      " 36 6745k   36 2432k    0     0   259k      0  0:00:25  0:00:09  0:00:16  408k\n",
      " 41 6745k   41 2816k    0     0   271k      0  0:00:24  0:00:10  0:00:14  426k\n",
      " 49 6745k   49 3312k    0     0   290k      0  0:00:23  0:00:11  0:00:12  448k\n",
      " 55 6745k   55 3712k    0     0   298k      0  0:00:22  0:00:12  0:00:10  429k\n",
      " 62 6745k   62 4192k    0     0   314k      0  0:00:21  0:00:13  0:00:08  455k\n",
      " 68 6745k   68 4592k    0     0   318k      0  0:00:21  0:00:14  0:00:07  425k\n",
      " 73 6745k   73 4976k    0     0   325k      0  0:00:20  0:00:15  0:00:05  439k\n",
      " 81 6745k   81 5488k    0     0   333k      0  0:00:20  0:00:16  0:00:04  428k\n",
      " 87 6745k   87 5872k    0     0   338k      0  0:00:19  0:00:17  0:00:02  439k\n",
      " 94 6745k   94 6368k    0     0   347k      0  0:00:19  0:00:18  0:00:01  435k\n",
      "100 6745k  100 6745k    0     0   352k      0  0:00:19  0:00:19 --:--:--  459k\n"
     ]
    }
   ],
   "source": [
    "!curl -o paper.pdf https://arxiv.org/pdf/2303.13519.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader # for loading the pdf\n",
    "from langchain.embeddings import OpenAIEmbeddings # for creating embeddings\n",
    "from langchain.vectorstores import Chroma # for the vectorization part\n",
    "from langchain.chains import ChatVectorDBChain # for chatting with the pdf\n",
    "from langchain.llms import OpenAI # the LLM model we'll use (CHatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning and Veriﬁcation of Task Structure in Instructional Videos\n",
      "Medhini Narasimhan1;2, Licheng Yu2, Sean Bell2, Ning Zhang2, Trevor Darrell1\n",
      "1UC Berkeley,2Meta AI\n",
      "https://medhini.github.io/task_structure\n",
      "Abstract\n",
      "Given the enormous number of instructional videos\n",
      "available online, learning a diverse array of multi-step task\n",
      "models from videos is an appealing goal. We introduce\n",
      "a new pre-trained video model, VideoTaskformer, focused\n",
      "on representing the semantics and structure of instructional\n",
      "videos. We pre-train VideoTaskformer using a simple and\n",
      "effective objective: predicting weakly supervised textual la-\n",
      "bels for steps that are randomly masked out from an instruc-\n",
      "tional video (masked step modeling). Compared to prior\n",
      "work which learns step representations locally, our ap-\n",
      "proach involves learning them globally, leveraging video of\n",
      "the entire surrounding task as context. From these learned\n",
      "representations, we can verify if an unseen video correctly\n",
      "executes a given task, as well as forecast which steps are\n",
      "likely to be taken after a given step. We introduce two new\n",
      "benchmarks for detecting mistakes in instructional videos,\n",
      "to verify if there is an anomalous step and if steps are exe-\n",
      "cuted in the right order. We also introduce a long-term fore-\n",
      "casting benchmark, where the goal is to predict long-range\n",
      "future steps from a given step. Our method outperforms pre-\n",
      "vious baselines on these tasks, and we believe the tasks will\n",
      "be a valuable way for the community to measure the quality\n",
      "of step representations. Additionally, we evaluate Video-\n",
      "Taskformer on 3 existing benchmarks—procedural activity\n",
      "recognition, step classiﬁcation, and step forecasting—and\n",
      "demonstrate on each that our method outperforms existing\n",
      "baselines and achieves new state-of-the-art performance.\n",
      "1. Introduction\n",
      "Picture this, you’re trying to build a bookshelf by watch-\n",
      "ing a YouTube video with several intricate steps. You’re\n",
      "annoyed by the need to repeatedly hit pause on the video\n",
      "and you’re unsure if you have gotten all the steps right so\n",
      "far. Fortunately, you have an interactive assistant that can\n",
      "guide you through the task at your own pace, verifying each\n",
      "\u0003Work done while an intern at Meta AI. Correspondence to\n",
      "medhini@berkeley.edu\n",
      "“Dip bread in batter”“Serve with maple syrup”\n",
      "EV1EV2EV3EV12\n",
      "Mask\n",
      "MaskVideoTaskformerT1T2T3T12Prediction over Step Classes\n",
      "“Dip bread in batter”EV1Prior work: Single clip step predictionOurs:Masked step prediction over all clips in video Figure 1: Prior work [13, 12] learns step representations from sin-\n",
      "gle short video clips, independent of the task, thus lacking knowl-\n",
      "edge of task structure. Our model, VideoTaskformer, learns step\n",
      "representations for masked video steps through the global context\n",
      "of all surrounding steps in the video, making our learned represen-\n",
      "tations aware of task semantics and structure.\n",
      "step as you perform it and interrupting you if you make a\n",
      "mistake. A composite task such as “ making a bookshelf ”\n",
      "involves multiple ﬁne-grained activities such as “ drilling\n",
      "holes ” and “ adding support blocks .” Accurately categoriz-\n",
      "ing these activities requires not only recognizing the indi-\n",
      "vidual steps that compose the task but also understanding\n",
      "the task structure, which includes the temporal ordering of\n",
      "the steps and multiple plausible ways of executing a step\n",
      "(e.g., one can beat eggs with a fork or a whisk). An ideal\n",
      "interactive assistant has both a high-level understanding of\n",
      "a broad range of tasks, as well as a low-level understanding\n",
      "of the intricate steps in the tasks, their temporal ordering,\n",
      "and the multiple ways of performing them.\n",
      "As seen in Fig. 1, prior work [12, 13] models step rep-\n",
      "resentations of a single step independent of the overall task\n",
      "context. This might not be the best strategy, given that steps\n",
      "for a task are related, and the way a step is situated in an\n",
      "overall task may contain important information about the\n",
      "step. To address this, we pre-train our model with a masked\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"./paper.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './paper.pdf', 'page': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m \u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage_content\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m      Interface for interacting with a document.\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "\u001b[1;31mFile:\u001b[0m           d:\\ml projects\\mlvenv\\lib\\site-packages\\langchain\\schema.py\n",
      "\u001b[1;31mType:\u001b[0m           ModelMetaclass\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key='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',max_retries=1)\n",
    "vectordb = Chroma.from_documents(pages, embedding=embeddings,\n",
    "                                 persist_directory=\".\")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML Projects\\mlvenv\\Lib\\site-packages\\langchain\\llms\\openai.py:189: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "d:\\ML Projects\\mlvenv\\Lib\\site-packages\\langchain\\llms\\openai.py:769: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "d:\\ML Projects\\mlvenv\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:250: UserWarning: `ChatVectorDBChain` is deprecated - please use `from langchain.chains import ConversationalRetrievalChain`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "VideoTaskformer is a pre-trained video model that focuses on representing the semantics and structure of instructional videos. It learns task-aware step representations from a large corpus of instructional videos using a BERT-style masked modeling loss. The learned step representations are \"context-aware\" and possess \"global\" knowledge of task-structure, making them aware of task semantics and structure. VideoTaskformer can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. It outperforms previous baselines on various benchmarks such as detecting mistakes in instructional videos, procedural activity recognition, step classification, and step forecasting.\n"
     ]
    }
   ],
   "source": [
    "pdf_qa = ChatVectorDBChain.from_llm(OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo\", openai_api_key='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'),\n",
    "                                    vectordb, return_source_documents=True)\n",
    "\n",
    "query = \"What is the VideoTaskformer?\"\n",
    "result = pdf_qa({\"question\": query, \"chat_history\": \"\"})\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.document_loaders.pdf.PyPDFLoader"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
